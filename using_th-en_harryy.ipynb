{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B_xVLVRugHLQ"
   },
   "source": [
    "# Login HuggingFace to Upload Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "iF67agq1kcA9"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cef4ceb0aaf435699e1557ed12e8246",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center>\\n<img src=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!git config --global credential.helper store\n",
    "\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.17.0\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric, DatasetDict, Dataset\n",
    "metric = load_metric(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def pre_process_from_csv(path, n_row=100000):\n",
    "    df_5M = pd.read_csv(path)\n",
    "    list_5M = df_5M.to_dict('records')[:n_row]\n",
    "    list_sub = ['LST_Corpus']*len(list_5M)\n",
    "    dict_5M = pd.DataFrame({\"translation\": list_5M, \"subdataset\": list_sub})\n",
    "    return dict_5M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = pre_process_from_csv('TEST_dataset_5M.csv', 100000)\n",
    "\n",
    "cut_datasets = DatasetDict()\n",
    "cut_datasets = Dataset.from_pandas(raw_datasets, split=\"train+validation\").train_test_split(0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['translation', 'subdataset'],\n",
       "        num_rows: 80000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['translation', 'subdataset'],\n",
       "        num_rows: 20000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cut_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mcut_datasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush_to_hub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mrepo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mprivate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtoken\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbranch\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNoneType\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mshard_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m524288000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Pushes the ``DatasetDict`` to the hub.\n",
       "The ``DatasetDict`` is pushed using HTTP requests and does not need to have neither git or git-lfs installed.\n",
       "\n",
       "Each dataset split will be pushed independently. The pushed dataset will keep the original split names.\n",
       "\n",
       "Args:\n",
       "    repo_id (:obj:`str`):\n",
       "        The ID of the repository to push to in the following format: `<user>/<dataset_name>` or\n",
       "        `<org>/<dataset_name>`. Also accepts `<dataset_name>`, which will default to the namespace\n",
       "        of the logged-in user.\n",
       "    private (Optional :obj:`bool`):\n",
       "        Whether the dataset repository should be set to private or not. Only affects repository creation:\n",
       "        a repository that already exists will not be affected by that parameter.\n",
       "    token (Optional :obj:`str`):\n",
       "        An optional authentication token for the Hugging Face Hub. If no token is passed, will default\n",
       "        to the token saved locally when logging in with ``huggingface-cli login``. Will raise an error\n",
       "        if no token is passed and the user is not logged-in.\n",
       "    branch (Optional :obj:`str`):\n",
       "        The git branch on which to push the dataset.\n",
       "    shard_size (Optional :obj:`int`):\n",
       "        The size of the dataset shards to be uploaded to the hub. The dataset will be pushed in files\n",
       "        of the size specified here, in bytes.\n",
       "\n",
       "Example:\n",
       "    .. code-block:: python\n",
       "\n",
       "        >>> dataset_dict.push_to_hub(\"<organization>/<dataset_id>\")\n",
       "\u001b[0;31mFile:\u001b[0m      /usr/local/lib/python3.8/dist-packages/datasets/dataset_dict.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?cut_datasets.push_to_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pushing split train to the Hub.\n",
      "The repository already exists: the `private` keyword argument will be ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a852d4bfb0f4cc9acabd31299f256b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pushing split test to the Hub.\n",
      "The repository already exists: the `private` keyword argument will be ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "797fa701d8f84994b8494d26566263c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cut_datasets.push_to_hub(repo_id=\"huak95/TNANA_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Git hooks.\n",
      "Git LFS initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'TNANA'...\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82b286d0a3804bd7a4c7df4e4dc8a9a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/80 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88651c12845d4456ab30e48cf1c51573",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/20 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cut_datasets.save_to_disk('fuck_thth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "model_checkpoint = \"Helsinki-NLP/opus-mt-th-en\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_checkpoint in [\"t5-small\", \"t5-base\", \"t5-larg\", \"t5-3b\", \"t5-11b\"]:\n",
    "    prefix = \"translate English to Romanian: \"\n",
    "else:\n",
    "    prefix = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_len  = 128\n",
    "max_target_len = 128\n",
    "\n",
    "source_lang = 'th'\n",
    "target_lang = 'en'\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + ex[source_lang] for ex in examples['translation']]\n",
    "    targets = []\n",
    "    for ex_ in examples['translation']:\n",
    "        ex = ex_[source_lang]\n",
    "        if ex is not None:\n",
    "            targets.append(ex)\n",
    "    \n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_len, truncation=True) # Pad to longest word (128 char)\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        try:\n",
    "            labels = tokenizer(targets, max_length=max_target_len, truncation=True)\n",
    "        except:\n",
    "            print('targets: ', targets)\n",
    "\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "987ef046dc7346a7a4b18b47bc9ce9e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29b8b800480241b5b1fdf6000337585d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = cut_datasets.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['translation', 'subdataset', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 80000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['translation', 'subdataset', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 20000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x8GqEX7grMTu"
   },
   "source": [
    "# 2. Using Translation Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mbLxzsQFv6Ct"
   },
   "source": [
    "### Using AutoModel (With model selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Install_Library.ipynb',\n",
       " '.git',\n",
       " 'Train_th-en_harryy_scb.ipynb',\n",
       " 'README.md',\n",
       " '.ipynb_checkpoints',\n",
       " 'LICENSE',\n",
       " 'huak95',\n",
       " 'revision.txt',\n",
       " 'Train_th-en_harryy.ipynb',\n",
       " '.gitattributes',\n",
       " 'small_100.csv',\n",
       " 'git_save.ipynb',\n",
       " 'TEST_dataset_5M.csv',\n",
       " 'opus-mt-th-en-finetuned-5k-th-to-en',\n",
       " 'using_th-en_harryy.ipynb']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "5f4c398c1f10479a9e593488e0288e13",
      "7379aad4cd5e443d87b8847c774730dd",
      "96bc72f5d2ab456c82ef93b9d1daae28",
      "9b9da8145682432b9e5c2c06fca1c7bd",
      "4a3154f8a418429d878d1ee69a23c384",
      "7db78072ae744fed884109b073112f16",
      "689c1ba0fe78498ab93862605dc13dfb",
      "579336e1b91a46f4b71a97166f7ad9bb",
      "cb1beb192231451a91497cce0537c10f",
      "0fe56995037840feb8d1b190a702dc53",
      "f7b13cea9b6f488fab4038319465b9f2",
      "0c03cf489288455ca73c7e9908bf93d6",
      "34c5e1e8d63d45a78593df8c4726a2af",
      "15baaa76664f41bcaa2b3d48368ea13b",
      "200f4e3d4c7445089bf84db41d02389d",
      "90786043cba94b52bf6a69676d84f49a",
      "d5173325310e4d36b217ef2c408b9784",
      "1178ab5e4a57412191c45a89f4d899c0",
      "fca6ce728fde4cdd8a819bdf558b163b",
      "bd4f8116bb0846a7ae8ed63159da24f5",
      "25c37fe96d68472b9bbfb6bd18c628de",
      "4c21a738a5284584b12bf3ca177579a4",
      "ba5f236325d04fa3bc297bd830c7af18",
      "1948d677fba34c9ba5bdbacc6dc8d5f0",
      "415afa813b014861ba9f082adfa83a91",
      "b56c0437a66e411ab4d169f02a211a67",
      "4deba2d529ce455684e4b6e059edab41",
      "9e5b9c3e27e146a0aadc98d241fe690b",
      "de715baa70c84245813a93c75c620118",
      "d63d0de90900413997bdb843657f56a0",
      "db58ea79429f48448809dbe9cdc4c8c9",
      "06495f9f21d547f385b1f6ff2df634b2",
      "efce4e270011493db42c4950a3f6e91f",
      "b2012e1abb7d4dfe87f00c80592c1d83",
      "7a7d6b8d8196415398311188ea6582fc",
      "f41f7e25c51b44c69dc2f05a8e06750b",
      "d75cf9a5d4204a27aa9f7707e8e202f6",
      "5cdd3f171ae14516b216397c45fc348e",
      "71957595cfd2480189d994a242050a8b",
      "674d82fe3edb4705b0a0a4cbafc22a71",
      "c190d9118f774c94a163261596bd83e8",
      "f16d43478f1145ccbe046d1aa6ad7a01",
      "c548e8569af8409ab05600aa746eaf29",
      "6f26805251bb461b85543ad11de84f9e"
     ]
    },
    "id": "6Lx1d4UBtvod",
    "outputId": "24c869ab-7c78-4741-8a1c-2b3e37833079",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "model_pretrain = \"Helsinki-NLP/opus-mt-th-en\"\n",
    "# model_for_predict = \"./opus-mt-th-en-finetuned-5k-th-to-en\"\n",
    "model_for_predict = \"huak95/opus-mt-th-en-finetuned-5k-th-to-en\"\n",
    "\n",
    "model_pt = AutoModelForSeq2SeqLM.from_pretrained(model_pretrain)\n",
    "model    = AutoModelForSeq2SeqLM.from_pretrained(model_for_predict)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_pretrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git lfs track \"opus-mt-th-en-finetuned-5k-th-to-en/*\"\n",
    "\n",
    "# model.push_to_hub('huak95/opus-mt-th-en-finetuned-5k-th-to-en')\n",
    "# # tokenizer.push_to_hub('huak95/opus-mt-th-en-finetuned-5k-th-to-en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def harry_translate(input_texts, model=model_pt):\n",
    "    inputs = tokenizer(input_texts['th'], return_tensors = \"pt\")\n",
    "\n",
    "    outputs = model.generate(inputs[\"input_ids\"],\n",
    "                             max_length=40,\n",
    "                             num_beams=4,\n",
    "                             early_stopping=True)\n",
    "\n",
    "    return tokenizer.decode(outputs[0]).replace('<pad> ',''), input_texts['en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'en': 'he wears glasses.', 'th': 'เขา ใส่ แว่น .'},\n",
       " {'en': 'kate this is kaoru', 'th': 'เค ท นี่ คือ คา โอ รุ'}]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets['test']['translation'][0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "pv48je1-uYIi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thai_text: เอา ล่ะ\n",
      "real_text:  here we go\n",
      "pred_text:  เอา และ\n",
      "pre_train:  All right.\n",
      "\n",
      "thai_text: ฉัน ไป ทำ ธุระ ที่นั่น ทุกๆ สาม เดือน .\n",
      "real_text:  i go there on business every three months.\n",
      "pred_text:  ฉัน ไป ทํา ส่วน ที่นั่นที่ ทุกหลัง สาม \n",
      "pre_train:  I do business there every three months.\n",
      "\n",
      "thai_text: โปรด ตัด ผม ออก จาก แผนการ ของ คุณ ที่ จะไป เล่น สกี ใน วัน สุดสัปดาห์ นี้\n",
      "real_text:  please count me out of your plans to go skiing for the weekend .\n",
      "pred_text:  โปรด ตั้ง ผม ออก จาก แรงการ ของ คุณ ที่\n",
      "pre_train:  Please cut me off from your plans to go skiing this weekend\n",
      "\n",
      "thai_text: ขอบคุณ กรุณา มา อีก\n",
      "real_text:  thank you very much please come again\n",
      "pred_text:  ขอบคุณ กรุณา มา อีก\n",
      "pre_train:  Thank you. Please come again.\n",
      "\n",
      "thai_text: เม็ด เหงื่อ ผุด ขึ้น บน หน้าผาก ของ เธอ\n",
      "real_text:  beads of perspiration gathered in her on her forehead\n",
      "pred_text:  และ น้ําหรับ แข่ ขึ้น บน หมาย ของ เธอ\n",
      "pre_train:  The sweatdrops on her forehead\n",
      "\n",
      "thai_text: วาด อิสระ\n",
      "real_text:  free hand\n",
      "pred_text:  แม้ อย่าง\n",
      "pre_train:  Draw Freedom\n",
      "\n",
      "thai_text: ฉัน ขอ ชีส เค้ก หนึ่ง ชิ้น ได้ไหม ?\n",
      "real_text:  could i have a slice of cheese cake?\n",
      "pred_text:  ฉัน ขอ เสีย คิด หนึ่ง ช่วย ได้ไหม?\n",
      "pre_train:  Can I have a piece of cheese cake?\n",
      "\n",
      "thai_text: คอมพิวเตอร์ นี้ ไม่ ทำงาน ถูกต้อง\n",
      "real_text:  this computer isn't working right\n",
      "pred_text:  เครื่องครั้ง นี้ ไม่ ทํางาน ถูก\n",
      "pre_train:  This computer doesn't work right.\n",
      "\n",
      "thai_text: มาก เกิน ไป\n",
      "real_text:  go over the odds\n",
      "pred_text:  มาก เกิน ไป\n",
      "pre_train:  Too Much\n",
      "\n",
      "thai_text: หน่อย ตะโกน ใส่ ป้อม ว่า \" นาย นี่ โง่ จัง ทำ อะไร ก็ ไม่ ถูก เหรอ \"\n",
      "real_text:  noi yelled at pom \" you ’ re a pinhead. don ’ t you know how to do anything right ? \"\n",
      "pred_text:  ห้อง กระเสียง ใส่ แข่ง ว่า \" นุ่ง นี้ \n",
      "pre_train:  And cried unto the strong, saying, Thou art foolish: is it not right that thou doest so?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(100,110):\n",
    "    text_ = tokenized_datasets['test']['translation'][i]\n",
    "    thai_text = text_['th']\n",
    "    pred_text_pt, real_text = harry_translate(text_, model_pt)\n",
    "    pred_text, real_text    = harry_translate(text_, model)\n",
    "    \n",
    "#     m_score = compute_metrics.compute(predictions=[pred_text], references=[real_text])\n",
    "\n",
    "    print('thai_text:',  thai_text)\n",
    "    print('real_text: ', real_text)\n",
    "    print('pred_text: ', pred_text)\n",
    "    print('pre_train: ', pred_text_pt)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some fucking git push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Install_Library.ipynb\t      git_save.ipynb\n",
      "LICENSE\t\t\t      huak95\n",
      "README.md\t\t      opus-mt-th-en-finetuned-5k-th-to-en\n",
      "TEST_dataset_5M.csv\t      revision.txt\n",
      "TNANA\t\t\t      small_100.csv\n",
      "Train_th-en_harryy.ipynb      test\n",
      "Train_th-en_harryy_scb.ipynb  train\n",
      "dataset_dict.json\t      using_th-en_harryy.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5de8d8aac974e81a2e72693fef30935",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/80 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "099066af7f574fde95bf10d46dbca071",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/20 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cut_datasets.save_to_disk('fuck_thth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "git lfs install\n",
    "git clone https://huggingface.co/datasets/huak95/TNANA\n",
    "# if you want to clone without large files – just their pointers\n",
    "# prepend your git clone with the following env var:\n",
    "GIT_LFS_SKIP_SMUDGE=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main 301d497] Add THANA\n",
      " 7 files changed, 0 insertions(+), 0 deletions(-)\n",
      " rename fuck_thth/dataset_dict.json => dataset_dict.json (100%)\n",
      " rename {fuck_thth/test => test}/dataset.arrow (100%)\n",
      " rename {fuck_thth/test => test}/dataset_info.json (100%)\n",
      " rename {fuck_thth/test => test}/state.json (100%)\n",
      " rename {fuck_thth/train => train}/dataset.arrow (100%)\n",
      " rename {fuck_thth/train => train}/dataset_info.json (100%)\n",
      " rename {fuck_thth/train => train}/state.json (100%)\n",
      "README.md\n",
      "dataset_dict.json\n",
      "test\n",
      "train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://huggingface.co/datasets/huak95/TNANA\n",
      "   b1e14dc..301d497  main -> main\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd TNANA\n",
    "git add .\n",
    "git commit -m 'Add THANA'\n",
    "git config --global user.name \"huak95\"\n",
    "git push\n",
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Password for 'https://huak95@github.com': "
     ]
    }
   ],
   "source": [
    "!git push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
